{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3b1ac2a-13fd-4f02-8840-481ab391fe53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2023-03-28 → 2023-04-27\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:21<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2023-04-28 → 2023-05-28\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:37<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2023-05-29 → 2023-06-28\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:30<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2023-06-29 → 2023-07-29\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:28<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2023-07-30 → 2023-08-29\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:31<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2023-08-30 → 2023-09-29\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:32<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2023-09-30 → 2023-10-30\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:07<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2023-10-31 → 2023-11-30\n",
      "This is a large query, it may take a moment to complete\n",
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:03<00:00,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2023-12-01 → 2023-12-31\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2024-01-01 → 2024-01-31\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2024-02-01 → 2024-03-02\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2024-03-03 → 2024-04-02\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:12<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2024-04-03 → 2024-05-03\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:33<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2024-05-04 → 2024-06-03\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:28<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2024-06-04 → 2024-07-04\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:30<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2024-07-05 → 2024-08-04\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:29<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2024-08-05 → 2024-09-04\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:33<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2024-09-05 → 2024-10-05\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:27<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2024-10-06 → 2024-11-05\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:07<00:00,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2024-11-06 → 2024-12-06\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:02<00:00,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2024-12-07 → 2025-01-06\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2025-01-07 → 2025-02-06\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2025-02-07 → 2025-03-09\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2025-03-10 → 2025-04-09\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping offseason dates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:21<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2025-04-10 → 2025-05-10\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:26<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2025-05-11 → 2025-06-10\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:29<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2025-06-11 → 2025-07-11\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:25<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2025-07-12 → 2025-08-11\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:20<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2025-08-12 → 2025-09-11\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:33<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 2025-09-12 → 2025-09-28\n",
      "This is a large query, it may take a moment to complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 17/17 [00:18<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from pybaseball import statcast\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def load_statcast_range(start_date, end_date, step_days=7):\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end   = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "    all_dfs = []\n",
    "    current = start\n",
    "\n",
    "    while current <= end:\n",
    "        chunk_start = current.strftime(\"%Y-%m-%d\")\n",
    "        chunk_end = min(current + timedelta(days=step_days), end).strftime(\"%Y-%m-%d\")\n",
    "        print(f\"Downloading: {chunk_start} → {chunk_end}\")\n",
    "\n",
    "        try:\n",
    "            chunk = statcast(start_dt=chunk_start, end_dt=chunk_end)\n",
    "            if chunk is not None and not chunk.empty:\n",
    "                all_dfs.append(chunk)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed on {chunk_start} → {chunk_end}: {e}\")\n",
    "\n",
    "        current += timedelta(days=step_days + 1)\n",
    "\n",
    "    if len(all_dfs) == 0:\n",
    "        raise ValueError(\"No data downloaded — check date ranges.\")\n",
    "    return pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "df = load_statcast_range(\"2023-03-28\", \"2025-09-28\", step_days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24f7ed94-e8c4-4f38-8ecc-87d1dfa0a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"game_type\"] == \"R\"]\n",
    "df = df[df['description'] == 'hit_into_play']\n",
    "df = df.dropna(subset=[\"launch_angle\", \"launch_speed\"])\n",
    "\n",
    "df = df.dropna(subset=[\"bat_speed\", \"swing_length\", \"attack_angle\"])\n",
    "\n",
    "numeric_features = [\n",
    "    \"release_speed\", \"release_spin_rate\", \"pfx_x\", \"pfx_z\",\n",
    "    \"plate_x\", \"plate_z\", \"vx0\", \"vy0\", \"vz0\",\n",
    "    \"ax\", \"ay\", \"az\",\n",
    "    \"bat_speed\", \"swing_length\", \"attack_angle\", \"attack_direction\",\n",
    "    \"estimated_slg_using_speedangle\", \"estimated_ba_using_speedangle\",\n",
    "    \"estimated_woba_using_speedangle\", \"woba_value\", \"babip_value\", \"iso_value\",\n",
    "    \"bat_score\", \"bat_score_diff\", \"bat_win_exp\", \"age_bat\", \n",
    "    \"batter_days_since_prev_game\", \"n_priorpa_thisgame_player_at_bat\"\n",
    "]\n",
    "\n",
    "categorical_features = [\"pitch_type\", \"stand\", \"p_throws\", \"home_team\", \"away_team\"]\n",
    "\n",
    "numeric_features = [f for f in numeric_features if f in df.columns]\n",
    "categorical_features = [f for f in categorical_features if f in df.columns]\n",
    "\n",
    "df[\"bat_speed_sq\"] = df[\"bat_speed\"] ** 2\n",
    "df[\"plane_match\"] = df[\"attack_angle\"] * df[\"pfx_z\"]\n",
    "df[\"vertical_offset\"] = df[\"plate_z\"] - (df[\"attack_angle\"] / 8)\n",
    "df[\"timing_offset\"] = df[\"vx0\"] / df[\"release_speed\"]\n",
    "\n",
    "numeric_features += [\"bat_speed_sq\", \"plane_match\", \"vertical_offset\", \"timing_offset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57597503-01f8-4495-b183-6eed7604f232",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "TARGET_LA = \"launch_angle\"\n",
    "TARGET_EV = \"launch_speed\"\n",
    "\n",
    "X_train = train_df[numeric_features + categorical_features].copy()\n",
    "X_test  = test_df[numeric_features + categorical_features].copy()\n",
    "\n",
    "y_train_la = train_df[TARGET_LA]\n",
    "y_train_ev = train_df[TARGET_EV]\n",
    "y_test_la = test_df[TARGET_LA]\n",
    "y_test_ev = test_df[TARGET_EV]\n",
    "\n",
    "X_train = pd.get_dummies(X_train, columns=categorical_features)\n",
    "X_test = pd.get_dummies(X_test, columns=categorical_features)\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train[numeric_features] = imputer.fit_transform(X_train[numeric_features])\n",
    "X_test[numeric_features] = imputer.transform(X_test[numeric_features])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n",
    "X_test[numeric_features] = scaler.transform(X_test[numeric_features])\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81aba2d6-98c4-4a30-9bbd-6e7714690f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(features, target):\n",
    "    X = torch.tensor(features.values, dtype=torch.float32)\n",
    "    y = torch.tensor(target.values, dtype=torch.float32).unsqueeze(1)\n",
    "    return X, y\n",
    "\n",
    "X_train_tensor, y_train_la_tensor = to_tensor(X_train, y_train_la)\n",
    "_, y_train_ev_tensor = to_tensor(X_train, y_train_ev)\n",
    "X_test_tensor, y_test_la_tensor = to_tensor(X_test, y_test_la)\n",
    "_, y_test_ev_tensor = to_tensor(X_test, y_test_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3927621-e2ad-4e1f-b5d6-1270f3947192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 1008.561\n",
      "Epoch 2/50: 1001.831\n",
      "Epoch 3/50: 990.783\n",
      "Epoch 4/50: 973.803\n",
      "Epoch 5/50: 952.506\n",
      "Epoch 6/50: 926.908\n",
      "Epoch 7/50: 897.678\n",
      "Epoch 8/50: 864.358\n",
      "Epoch 9/50: 829.403\n",
      "Epoch 10/50: 794.436\n",
      "Epoch 11/50: 762.051\n",
      "Epoch 12/50: 735.532\n",
      "Epoch 13/50: 715.739\n",
      "Epoch 14/50: 702.712\n",
      "Epoch 15/50: 692.636\n",
      "Epoch 16/50: 684.340\n",
      "Epoch 17/50: 677.524\n",
      "Epoch 18/50: 671.182\n",
      "Epoch 19/50: 664.766\n",
      "Epoch 20/50: 659.884\n",
      "Epoch 21/50: 654.759\n",
      "Epoch 22/50: 649.591\n",
      "Epoch 23/50: 644.740\n",
      "Epoch 24/50: 639.176\n",
      "Epoch 25/50: 634.248\n",
      "Epoch 26/50: 629.801\n",
      "Epoch 27/50: 624.820\n",
      "Epoch 28/50: 619.869\n",
      "Epoch 29/50: 614.702\n",
      "Epoch 30/50: 610.346\n",
      "Epoch 31/50: 605.477\n",
      "Epoch 32/50: 600.940\n",
      "Epoch 33/50: 595.873\n",
      "Epoch 34/50: 590.764\n",
      "Epoch 35/50: 586.607\n",
      "Epoch 36/50: 582.099\n",
      "Epoch 37/50: 577.822\n",
      "Epoch 38/50: 573.751\n",
      "Epoch 39/50: 569.111\n",
      "Epoch 40/50: 564.478\n",
      "Epoch 41/50: 560.137\n",
      "Epoch 42/50: 555.794\n",
      "Epoch 43/50: 551.235\n",
      "Epoch 44/50: 547.430\n",
      "Epoch 45/50: 543.067\n",
      "Epoch 46/50: 539.103\n",
      "Epoch 47/50: 534.901\n",
      "Epoch 48/50: 530.266\n",
      "Epoch 49/50: 527.004\n",
      "Epoch 50/50: 522.003\n",
      "Epoch 1/50: 8043.121\n",
      "Epoch 2/50: 8015.677\n",
      "Epoch 3/50: 7964.702\n",
      "Epoch 4/50: 7889.318\n",
      "Epoch 5/50: 7783.942\n",
      "Epoch 6/50: 7641.504\n",
      "Epoch 7/50: 7458.511\n",
      "Epoch 8/50: 7233.781\n",
      "Epoch 9/50: 6959.379\n",
      "Epoch 10/50: 6632.212\n",
      "Epoch 11/50: 6252.927\n",
      "Epoch 12/50: 5817.820\n",
      "Epoch 13/50: 5326.172\n",
      "Epoch 14/50: 4779.860\n",
      "Epoch 15/50: 4185.407\n",
      "Epoch 16/50: 3557.004\n",
      "Epoch 17/50: 2907.381\n",
      "Epoch 18/50: 2255.358\n",
      "Epoch 19/50: 1639.825\n",
      "Epoch 20/50: 1101.148\n",
      "Epoch 21/50: 687.105\n",
      "Epoch 22/50: 443.759\n",
      "Epoch 23/50: 350.865\n",
      "Epoch 24/50: 318.312\n",
      "Epoch 25/50: 298.229\n",
      "Epoch 26/50: 283.768\n",
      "Epoch 27/50: 272.253\n",
      "Epoch 28/50: 264.488\n",
      "Epoch 29/50: 256.257\n",
      "Epoch 30/50: 251.544\n",
      "Epoch 31/50: 244.992\n",
      "Epoch 32/50: 240.688\n",
      "Epoch 33/50: 235.872\n",
      "Epoch 34/50: 232.270\n",
      "Epoch 35/50: 228.280\n",
      "Epoch 36/50: 226.163\n",
      "Epoch 37/50: 222.966\n",
      "Epoch 38/50: 219.687\n",
      "Epoch 39/50: 217.460\n",
      "Epoch 40/50: 214.919\n",
      "Epoch 41/50: 212.752\n",
      "Epoch 42/50: 211.058\n",
      "Epoch 43/50: 209.261\n",
      "Epoch 44/50: 208.748\n",
      "Epoch 45/50: 205.912\n",
      "Epoch 46/50: 204.373\n",
      "Epoch 47/50: 202.852\n",
      "Epoch 48/50: 201.636\n",
      "Epoch 49/50: 200.805\n",
      "Epoch 50/50: 199.148\n"
     ]
    }
   ],
   "source": [
    "class MLPRegression(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def train_model(model, X, y, lr=1e-5, epochs=50, batch_size=1024):\n",
    "    loader = DataLoader(TensorDataset(X, y), batch_size=batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        for xb, yb in loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: {running_loss/len(loader):.3f}\")\n",
    "    return model\n",
    "\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model_la = MLPRegression(input_dim)\n",
    "model_ev = MLPRegression(input_dim)\n",
    "\n",
    "model_la = train_model(model_la, X_train_tensor, y_train_la_tensor)\n",
    "model_ev = train_model(model_ev, X_train_tensor, y_train_ev_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "974fb687-91a2-4f54-9ff0-dae25f1daeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Launch Angle RMSE: 22.57\n",
      "Exit Velocity RMSE: 12.16\n"
     ]
    }
   ],
   "source": [
    "def predict_with_uncertainty(model, X, n_samples=30):\n",
    "    model.train()\n",
    "    preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_samples):\n",
    "            preds_list.append(model(X).cpu().numpy())\n",
    "    preds_array = np.array(preds_list)\n",
    "    mean_preds = preds_array.mean(axis=0).flatten()\n",
    "    std_preds = preds_array.std(axis=0).flatten()\n",
    "    return mean_preds, std_preds\n",
    "\n",
    "pred_la_mean, pred_la_std = predict_with_uncertainty(model_la, X_test_tensor)\n",
    "pred_ev_mean, pred_ev_std = predict_with_uncertainty(model_ev, X_test_tensor)\n",
    "\n",
    "rmse_la = np.sqrt(mean_squared_error(y_test_la, pred_la_mean))\n",
    "rmse_ev = np.sqrt(mean_squared_error(y_test_ev, pred_ev_mean))\n",
    "print(f\"\\nLaunch Angle RMSE: {rmse_la:.2f}\")\n",
    "print(f\"Exit Velocity RMSE: {rmse_ev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ce43eb3-ef8d-40a9-9dbe-9a89a4dbb867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions saved to 'predicted_results_pytorch.xlsx'\n"
     ]
    }
   ],
   "source": [
    "predicted_df = test_df.copy()\n",
    "predicted_df[\"launch_angle_pred\"] = pred_la_mean\n",
    "predicted_df[\"launch_angle_std\"] = pred_la_std\n",
    "predicted_df[\"launch_speed_pred\"] = pred_ev_mean\n",
    "predicted_df[\"launch_speed_std\"] = pred_ev_std\n",
    "\n",
    "predicted_df.to_excel(\"predicted_results_pytorch.xlsx\", index=False)\n",
    "print(\"\\nPredictions saved to 'predicted_results_pytorch.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e77f247-edb1-4d5f-9d03-f9cf29828246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_season_summary(batter_id, df, X_df, model_la, model_ev, filter_bip=True):\n",
    "\n",
    "    player_df = df[df['batter'] == batter_id].copy()\n",
    "    \n",
    "    if filter_bip:\n",
    "        player_df = player_df[player_df['description'] == 'hit_into_play']\n",
    "    \n",
    "    if player_df.empty:\n",
    "        print(f\"No data found for batter ID {batter_id} with filter_bip={filter_bip}\")\n",
    "        return None\n",
    "    \n",
    "    actual_la = player_df['launch_angle'].mean()\n",
    "    actual_ev = player_df['launch_speed'].mean()\n",
    "    \n",
    "    X_player = X_df.loc[player_df.index].copy().astype(np.float32)\n",
    "    X_player_tensor = torch.tensor(X_player.values, dtype=torch.float32)\n",
    "    \n",
    "    pred_la_mean, pred_la_std = predict_with_uncertainty(model_la, X_player_tensor)\n",
    "    pred_ev_mean, pred_ev_std = predict_with_uncertainty(model_ev, X_player_tensor)\n",
    "    \n",
    "    predicted_la = pred_la_mean.mean()\n",
    "    predicted_ev = pred_ev_mean.mean()\n",
    "    \n",
    "    summary = pd.DataFrame({\n",
    "        \"Metric\": [\"Launch Angle\", \"Exit Velocity\"],\n",
    "        \"Actual\": [actual_la, actual_ev],\n",
    "        \"Predicted\": [predicted_la, predicted_ev],\n",
    "        \"Pred Std (avg)\": [pred_la_std.mean(), pred_ev_std.mean()]\n",
    "    })\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7f12fe1-9bff-49c5-a3b1-61dbbb1fba1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Metric     Actual  Predicted  Pred Std (avg)\n",
      "0   Launch Angle  15.739130  14.553982        3.870971\n",
      "1  Exit Velocity  89.502609  89.087524        7.427976\n"
     ]
    }
   ],
   "source": [
    "freeman_summary = player_season_summary(\n",
    "    batter_id=518692,  # Freddie Freeman\n",
    "    df=test_df,\n",
    "    X_df=X_test,\n",
    "    model_la=model_la,\n",
    "    model_ev=model_ev,\n",
    "    filter_bip=True\n",
    ")\n",
    "\n",
    "print(freeman_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cc1b07-4bd1-4d3d-9249-8c6934774b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
